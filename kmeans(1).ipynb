{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The k-Means Algorithm\n",
    "\n",
    "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n",
    "\n",
    "\n",
    "First example (educational) : we start by generating some artificial data ( you should comment each step):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "nb_clusters=randint(2,6)\n",
    "print(nb_clusters)\n",
    "import sklearn.datasets as datasets\n",
    "X, Y = datasets.make_blobs(n_samples=1000, centers=nb_clusters, cluster_std=0.5, center_box=(-10,10),random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we first plot the data to get a feeling of what we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like it may contain four different \"types\" of data point.\n",
    "\n",
    "In fact, this is how it was created above.\n",
    "\n",
    "We can plot this information as well, using color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, you do not know the information in Y, however.\n",
    "\n",
    "You could try to recover it from the data alone.\n",
    "\n",
    "This is what the kMeans algorithm does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(nb_clusters, random_state=8)\n",
    "Y_hat = kmeans.fit(X).labels_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the label assignments should be quite similar to Y, up to a different ordering of the colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=Y_hat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you're not so much interested in the assignments to the means.\n",
    "\n",
    "You'll want to have a closer look at the means μ.\n",
    "\n",
    "The means in μ can be seen as representatives of their respective cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=Y_hat, alpha=0.4)\n",
    "mu = kmeans.cluster_centers_\n",
    "plt.scatter(mu[:,0], mu[:,1], s=100, c=np.unique(Y_hat))\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 0.1 : in this case k (#nb_clusters) is known.  but what if it wasn't. how to determine the optimal number of clusters ?\n",
    "The variance quantity W_k is the basis of a naive procedure to determine the optimal number of clusters: \n",
    "The Elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.[1] Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.\n",
    "\n",
    "Documentation : https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#calculate the amount of variance for each value of K between 1 and 10 and plot it\n",
    "#some help repeat kmeans for each value of K between 1 and 10 and use kmeans.inertia_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## excercise 1: \n",
    "1-Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a clustering task: split the observations into well-separated 3 group called clusters.\n",
    "for more help : http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html\n",
    "\n",
    "2- for the iris dataset try to use the first two principal components like inputs for the clustring task.\n",
    "\n",
    "3- compare the two classifications. Is there any difference between 2 and 3?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## excercise 2: \n",
    "\n",
    "how we can use k-means for MNIST? \n",
    "\n",
    "### first approach :\n",
    "We know that there is 10 digits, so we could try a clustering task: split the observations into well-separated 3 group called clusters (for this use the dataset.test because it contains only 10000 digits)\n",
    "\n",
    "### Second approch :\n",
    "For only one digit try a segmentation image. for more information about it, go to : http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3: recommended for students of technical disciplines (eg mathematics)\n",
    "go to : http://lear.inrialpes.fr/people/mairal/teaching/2014-2015/M2ENS/notes_cours7.pdf\n",
    "read the first algorithm carefully and try to implement it step by step ( don't worry it's easy ;) )\n",
    "## Excercise 4: (just for manuela):\n",
    "according to the link in Excercise 3 ( specially the first theorem ) , in the k-means we try to minimize the inertia.\n",
    "can we proceed differently (using the gradient descent algorithm for example) to find the centroids ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final excercise : \n",
    "The k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n",
    "\n",
    "for more information about it : http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Talk.pdf\n",
    "\n",
    "how knn algorithm works : https://www.youtube.com/watch?v=UqYde-LULfs\n",
    "\n",
    "in python :  http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
